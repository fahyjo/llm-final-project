{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "Standard import failed for UnslothXPOTrainer: No module named 'UnslothXPOTrainer'. Using tempfile instead!\n"
     ]
    }
   ],
   "source": [
    "import unsloth\n",
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from typing import List, Dict, Tuple, Any\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from grpo import SYSTEM_PROMPT\n",
    "from tsp_llm import load_tsp_dataset\n",
    "from tsp import calculate_tsp_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.50.3.\n",
      "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.381 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu118. CUDA: 8.0. CUDA Toolkit: 11.8. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Will map <|im_end|> to EOS = <|eot_id|>.\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "  model_name=\"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "  max_seq_length=4096,\n",
    "  load_in_4bit=True,\n",
    "  dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Prepare model for inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Use 'chatml' chat template for Llama 3.1\n",
    "tokenizer = get_chat_template(\n",
    "  tokenizer,\n",
    "  chat_template=\"chatml\",\n",
    "  mapping={\n",
    "    \"role\":\"from\",\n",
    "    \"content\":\"value\",\n",
    "    \"user\":\"human\",\n",
    "    \"assistant\":\"gpt\"\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_assistant_response_regex(decoded_text: str) -> str:\n",
    "    \"\"\"Extract assistant response using regex pattern matching.\"\"\"\n",
    "    # Pattern to match assistant's response between markers\n",
    "    pattern = r\"<\\|im_start\\|>assistant\\n(.*?)(?:<\\|im_end\\|>|$)\"\n",
    "    \n",
    "    # Search with DOTALL to match across newlines\n",
    "    match = re.search(pattern, decoded_text, re.DOTALL)\n",
    "    \n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        # Return the original (might be direct generation without markers)\n",
    "        return decoded_text.strip()\n",
    "    \n",
    "def generate(prompt: str, max_new_tokens: int = 2048, num_samples: int = 1, temperature: float = 0.7, top_p: float = 0.9) -> Dict[str, Any]:\n",
    "  \"\"\"\n",
    "  Generate a response from the model based on the prompt.\n",
    "\n",
    "  Args:\n",
    "    prompt: The input prompt\n",
    "    max_new_tokens: Max number of tokenx to generate\n",
    "    num_samples: Number of samples per prompt\n",
    "    temperature: Model temperature\n",
    "    top_p: Model top_p\n",
    "  \n",
    "  Returns:\n",
    "    Dict[str, Any]: Summary dict containing input token count, average output token count, and\n",
    "                    model responses \n",
    "  \"\"\"\n",
    "\n",
    "  # Format chat message\n",
    "  messages = [\n",
    "    {\"from\": \"system\", \"value\": SYSTEM_PROMPT},\n",
    "    {\"from\": \"human\", \"value\": prompt}\n",
    "  ]\n",
    "\n",
    "  # Tokenize input and move input tensors to GPU\n",
    "  inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    "  ).to(\"cuda\")\n",
    "\n",
    "  # Get input token count\n",
    "  input_token_count = inputs.shape[1]\n",
    "\n",
    "  # Generate outputs\n",
    "  outputs = model.generate(\n",
    "    input_ids=inputs,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    num_return_sequences=num_samples,\n",
    "    do_sample=True,\n",
    "    temperature=temperature,\n",
    "    top_p=top_p,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True\n",
    "  )\n",
    "\n",
    "  # Get output token counts\n",
    "  output_sequences = outputs.sequences\n",
    "  avg_output_token_count = sum([len(output) - input_token_count for output in output_sequences]) / len(output_sequences)\n",
    "\n",
    "  # Process outputs\n",
    "  responses = [tokenizer.decode(output, skip_special_tokens=True) for output in output_sequences]\n",
    "  responses = [extract_assistant_response_regex(response) for response in responses]\n",
    "\n",
    "  # Return token counts and responses\n",
    "  out = {\n",
    "    \"input token count\": input_token_count,\n",
    "    \"average output token count\": avg_output_token_count,\n",
    "    \"responses\": responses\n",
    "  }\n",
    "\n",
    "  return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# String Parsing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_trace(response: str) -> List[int]:\n",
    "    \"\"\"\n",
    "    Extracts the last occurrence of a list of ints inside <trace></trace> or <trace><trace> brackets.\n",
    "\n",
    "    Args:\n",
    "        response: The model response\n",
    "    \n",
    "    Returns:\n",
    "        List[int]: The last trace provided by the model\n",
    "    \"\"\"\n",
    "\n",
    "    # Regex pattern to match <trace>...</trace> and <trace>...<trace>\n",
    "    matches = re.findall(r\"<trace>\\s*([\\d]+(?:\\s*,\\s*[\\d]+)*)\\s*</trace>|<trace>\\s*([\\d]+(?:\\s*,\\s*[\\d]+)*)\\s*<trace>\", response)\n",
    "    if not matches:\n",
    "        return []\n",
    "    \n",
    "    # Extract the last non-empty match\n",
    "    last_match = next(filter(None, matches[-1]))\n",
    "\n",
    "    # Convert to list of integers\n",
    "    return [int(num) for num in re.split(r\"\\s*,\\s*\", last_match)]\n",
    "\n",
    "def extract_total_length(prompt: str) -> List[int]:\n",
    "    \"\"\"\n",
    "    Extracts reference distances provided in prompt.\n",
    "\n",
    "    Args:\n",
    "      prompt: The model prompt\n",
    "\n",
    "    Returns:\n",
    "      List[int]: The reference distances\n",
    "    \"\"\"\n",
    "    return [int(match) for match in re.findall(r'total length:\\s*(\\d+)', prompt)]\n",
    "\n",
    "def extract_tsp(prompt: str) -> Dict[int, Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Extracts the tsp nodes from the prompt.\n",
    "\n",
    "    Args:\n",
    "        prompt: The user prompt\n",
    "\n",
    "    Returns:\n",
    "        Dict[int, Tuple[int, int]]: Nodes in i: x, y format where i is the node index, x is the x\n",
    "                                    cooridnate, and y is the y coordinate\n",
    "    \"\"\"\n",
    "    # Find the line that starts with \"Given\"\n",
    "    lines = prompt.split('\\n')\n",
    "    node_lines = []\n",
    "    \n",
    "    # Flag to track when we're in the nodes section\n",
    "    capturing = False\n",
    "    \n",
    "    # Find and collect the node lines\n",
    "    for line in lines:\n",
    "        if line.startswith(\"Given\"):\n",
    "            capturing = True\n",
    "            continue\n",
    "        \n",
    "        if capturing and line.startswith(\"Node:\"):\n",
    "            node_lines.append(line)\n",
    "        \n",
    "        # Stop capturing when we hit an empty line after finding nodes\n",
    "        if capturing and line.strip() == \"\":\n",
    "            break\n",
    "    \n",
    "    # If we didn't find any nodes through the first approach, try direct pattern matching\n",
    "    if not node_lines:\n",
    "        import re\n",
    "        node_pattern = r\"Node: (\\d+): \\((-?\\d+), (-?\\d+)\\)\"\n",
    "        node_lines = re.findall(node_pattern, prompt)\n",
    "        \n",
    "        # Convert regexp results to dictionary directly if we found matches\n",
    "        if node_lines:\n",
    "            return {int(node): (int(x), int(y)) for node, x, y in node_lines}\n",
    "    \n",
    "    # Process the node lines\n",
    "    tsp = {}\n",
    "    for line in node_lines:\n",
    "        # Extract node number and coordinates\n",
    "        parts = line.split(\": \")\n",
    "        node = int(parts[1].strip())\n",
    "        \n",
    "        # Extract coordinates - handling the parentheses\n",
    "        coords = parts[2].strip()\n",
    "        coords = coords.strip(\"()\")\n",
    "        x, y = map(int, coords.split(\", \"))\n",
    "        \n",
    "        tsp[node] = (x, y)\n",
    "    \n",
    "    return tsp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Reward Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def optimal_solution_reward_func(responses: List[str], optimal_distance: float, tsp: Dict[int, Tuple[int, int]]) -> List[float]:\n",
    "    \"\"\" Score = 2.0 if response solution matches optimal solution, 0.0 otherwise \"\"\"\n",
    "    valid_response_rewards = valid_response_reward_func(responses, len(tsp) + 1)\n",
    "    traces = [extract_trace(r) for r in responses]\n",
    "    distances = [calculate_tsp_distance(tsp, t) for t in traces]\n",
    "    return [2.0 if ((valid_response_rewards[i] == 1.0) and (distances[i] -.1 <= optimal_distance)) else 0.0 for i in range(len(responses))]\n",
    "\n",
    "def improvement_reward_func(responses: List[str], reference_distance: int, tsp: Dict[int, Tuple[int, int]]) -> List[float]:\n",
    "    \"\"\" Score = 2.0 if response solution improves on provided solutions, 0.0 otherwise \"\"\"\n",
    "    valid_response_rewards = valid_response_reward_func(responses, len(tsp) + 1)\n",
    "    traces = [extract_trace(r) for r in responses]\n",
    "    distances = [calculate_tsp_distance(tsp, t) for t in traces]\n",
    "    return [2.0 if ((valid_response_rewards[i] == 1.0) and (distances[i] < reference_distance)) else 0.0 for i in range(len(responses))]\n",
    "\n",
    "def valid_response_reward_func(responses: List[str], trace_length: int) -> List[float]:\n",
    "    \"\"\" Score = 1.0 if response solution contains trace with correct length, start node end node, and node set, 0.0 otherwise \"\"\"\n",
    "    traces = [extract_trace(r) for r in responses]\n",
    "    return [\n",
    "        1.0 if len(trace) == trace_length\n",
    "        and trace[0] == 0\n",
    "        and trace[-1] == 0\n",
    "        and set(trace) == set(range(trace_length - 1)) else 0.0 for trace in traces]\n",
    "\n",
    "def strict_format_reward_func(responses: List[str]) -> List[float]:\n",
    "    \"\"\" Score = 0.5 if response solution matches requested solution format, 0.0 otherwise \"\"\"\n",
    "    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<trace>\\n.*?\\n</trace>\\n$\"\n",
    "    matches = [re.match(pattern, r, flags=re.DOTALL) for r in responses] \n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "def soft_format_reward_func(responses: List[str]) -> List[float]:\n",
    "    \"\"\" Score = 0.5 if response solution loosely matches request solution format, 0.0 otherwise \"\"\"\n",
    "    pattern = r\"<reasoning>.*?</reasoning>\\s*<trace>.*?</trace>\"\n",
    "    matches = [re.match(pattern, r, flags=re.DOTALL) for r in responses] \n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "def apply_reward_functions(problem: Dict[str, Any], responses: str) -> Dict[str, float]:\n",
    "    n = len(responses)\n",
    "    optimal_solution_rewards = optimal_solution_reward_func(responses, problem['solution']['distance'], extract_tsp(problem['prompt']))\n",
    "    improvement_rewards = improvement_reward_func(responses, extract_total_length(problem['prompt'])[-1], extract_tsp(problem['prompt']))\n",
    "    valid_response_rewards = valid_response_reward_func(responses, problem['size'] + 1)\n",
    "    print(optimal_solution_rewards)\n",
    "    print(improvement_rewards)\n",
    "    print(valid_response_rewards)\n",
    "    strict_format_rewards = strict_format_reward_func(responses)\n",
    "    soft_format_rewards = soft_format_reward_func(responses)\n",
    "\n",
    "    return {\n",
    "        \"average optimal solution reward\": sum(optimal_solution_rewards) / n,\n",
    "        \"average improvement reward\": sum(improvement_rewards) / n,\n",
    "        \"average valid response reward\": sum(valid_response_rewards) / n,\n",
    "        \"average strict format reward\": sum(strict_format_rewards) / n,\n",
    "        \"average soft format reward\": sum(soft_format_rewards) / n\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Benchmark Dataset and Solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Solving TSP Benchmark:   0%|          | 0/60 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Solving TSP Benchmark:   2%|▏         | 1/60 [01:31<1:29:33, 91.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 2.0]\n",
      "[0.0, 0.0, 2.0]\n",
      "[1.0, 1.0, 1.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Solving TSP Benchmark:   3%|▎         | 2/60 [02:15<1:01:44, 63.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0]\n",
      "[2.0, 2.0, 2.0]\n",
      "[1.0, 1.0, 1.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Solving TSP Benchmark:   5%|▌         | 3/60 [02:49<47:31, 50.02s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 2.0, 0.0]\n",
      "[0.0, 2.0, 0.0]\n",
      "[1.0, 1.0, 1.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Solving TSP Benchmark:   7%|▋         | 4/60 [04:41<1:09:27, 74.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 2.0]\n",
      "[0.0, 0.0, 2.0]\n",
      "[1.0, 0.0, 1.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Solving TSP Benchmark:   8%|▊         | 5/60 [06:33<1:20:33, 87.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0]\n",
      "[1.0, 0.0, 0.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Solving TSP Benchmark:  10%|█         | 6/60 [07:53<1:16:52, 85.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0]\n",
      "[0.0, 2.0, 0.0]\n",
      "[1.0, 1.0, 1.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Solving TSP Benchmark:  12%|█▏        | 7/60 [08:49<1:06:55, 75.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.0, 0.0, 0.0]\n",
      "[2.0, 0.0, 0.0]\n",
      "[1.0, 1.0, 1.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Solving TSP Benchmark:  13%|█▎        | 8/60 [10:41<1:15:37, 87.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 2.0]\n",
      "[1.0, 0.0, 1.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Solving TSP Benchmark:  15%|█▌        | 9/60 [12:33<1:20:41, 94.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0]\n",
      "[0.0, 1.0, 1.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Solving TSP Benchmark:  17%|█▋        | 10/60 [13:03<1:02:25, 74.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0]\n",
      "[1.0, 0.0, 1.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Solving TSP Benchmark:  18%|█▊        | 11/60 [13:30<49:14, 60.30s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0]\n",
      "[2.0, 2.0, 0.0]\n",
      "[1.0, 1.0, 0.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Solving TSP Benchmark:  20%|██        | 12/60 [14:33<48:51, 61.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0]\n",
      "[1.0, 1.0, 1.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Solving TSP Benchmark:  22%|██▏       | 13/60 [15:44<50:17, 64.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 2.0]\n",
      "[0.0, 0.0, 2.0]\n",
      "[1.0, 1.0, 1.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Solving TSP Benchmark:  23%|██▎       | 14/60 [17:16<55:31, 72.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0]\n",
      "[1.0, 1.0, 1.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Solving TSP Benchmark:  25%|██▌       | 15/60 [18:48<58:43, 78.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0]\n",
      "[1.0, 1.0, 1.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Solving TSP Benchmark:  27%|██▋       | 16/60 [19:36<50:48, 69.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.0, 0.0, 0.0]\n",
      "[2.0, 2.0, 0.0]\n",
      "[1.0, 1.0, 1.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Solving TSP Benchmark:  28%|██▊       | 17/60 [21:28<58:48, 82.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0]\n",
      "[0.0, 1.0, 1.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Solving TSP Benchmark:  30%|███       | 18/60 [22:32<53:43, 76.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 2.0]\n",
      "[0.0, 0.0, 2.0]\n",
      "[1.0, 0.0, 1.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Solving TSP Benchmark:  32%|███▏      | 19/60 [23:07<43:56, 64.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 2.0, 0.0]\n",
      "[0.0, 0.0, 0.0]\n",
      "[1.0, 1.0, 1.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Solving TSP Benchmark:  33%|███▎      | 20/60 [24:07<41:59, 63.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0]\n",
      "[0.0, 1.0, 1.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Solving TSP Benchmark:  35%|███▌      | 21/60 [25:59<50:28, 77.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0]\n",
      "[1.0, 0.0, 1.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Solving TSP Benchmark:  37%|███▋      | 22/60 [26:44<42:54, 67.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0]\n",
      "[1.0, 1.0, 1.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Solving TSP Benchmark:  38%|███▊      | 23/60 [27:07<33:37, 54.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.0, 0.0, 0.0]\n",
      "[2.0, 0.0, 0.0]\n",
      "[1.0, 1.0, 1.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Solving TSP Benchmark:  40%|████      | 24/60 [27:42<29:02, 48.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0]\n",
      "[0.0, 2.0, 2.0]\n",
      "[0.0, 1.0, 1.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Solving TSP Benchmark:  42%|████▏     | 25/60 [28:07<24:09, 41.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 2.0]\n",
      "[2.0, 0.0, 2.0]\n",
      "[1.0, 1.0, 1.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Solving TSP Benchmark:  43%|████▎     | 26/60 [29:58<35:26, 62.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 2.0]\n",
      "[0.0, 0.0, 2.0]\n",
      "[1.0, 0.0, 1.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Solving TSP Benchmark:  45%|████▌     | 27/60 [30:37<30:23, 55.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 2.0]\n",
      "[1.0, 1.0, 1.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Solving TSP Benchmark:  47%|████▋     | 28/60 [32:04<34:33, 64.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0]\n",
      "[1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# Load benchmark prompt dataset\n",
    "dataset = load_tsp_dataset(\"tsp_benchmark_prompt_dataset.json\")\n",
    "\n",
    "pbar = tqdm(total=60, desc=\"Solving TSP Benchmark\")\n",
    "\n",
    "results = {\n",
    "  \"size_5\": [],\n",
    "  \"size_10\": [],\n",
    "  \"size_15\": []\n",
    "}\n",
    "\n",
    "# Solve benchmark dataset\n",
    "for i in range(len(dataset)):\n",
    "  if i == 10: i += 20\n",
    "  if i == 40: i += 20\n",
    "  if i == 70: break\n",
    "\n",
    "  problem = dataset[i]\n",
    "  size_key = f\"size_{problem['size']}\"\n",
    "  prompt = problem['prompt']\n",
    "  solution = problem['solution']\n",
    "\n",
    "  # Generate 3 completions per prompt\n",
    "  generate_out = generate(prompt, num_samples=3)\n",
    "\n",
    "  # Calculate average rewards across 3 samples\n",
    "  rewards = apply_reward_functions(problem, generate_out['responses'])\n",
    "\n",
    "  # Remove completions from gen out\n",
    "  del generate_out['responses']\n",
    "\n",
    "  # Append problem summary to results\n",
    "  generate_out.update(rewards)\n",
    "  problem_summary = generate_out\n",
    "  results[size_key].append(problem_summary)\n",
    "\n",
    "  # Update progress bar\n",
    "  pbar.update(1)\n",
    "\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Results and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary = {}\n",
    "\n",
    "for size in results:\n",
    "  size_summary = {}\n",
    "  n = len(results[size]) or 1\n",
    "\n",
    "  # Calculate summary per size\n",
    "  average_input_token_count = sum([problem['input token count'] for problem in results[size]]) / n\n",
    "  average_output_token_count = sum([problem['average output token count'] for problem in results[size]]) / n\n",
    "  average_optimal_solution_reward = sum([problem['average optimal solution reward'] for problem in results[size]]) / n\n",
    "  average_improved_reward = sum([problem['average improvement reward'] for problem in results[size]]) / n\n",
    "  average_valid_response_reward = sum([problem['average valid response reward'] for problem in results[size]]) / n\n",
    "  average_strict_format_reward = sum([problem['average strict format reward'] for problem in results[size]]) / n\n",
    "  average_soft_format_reward = sum([problem['average soft format reward'] for problem in results[size]]) / n\n",
    "\n",
    "  # Load size summary\n",
    "  size_summary['average input token count'] = average_input_token_count\n",
    "  size_summary['average output token count'] = average_output_token_count\n",
    "  size_summary['average optimal solution reward'] = average_optimal_solution_reward\n",
    "  size_summary['average improvement reward'] = average_improved_reward\n",
    "  size_summary['average valid response reward'] = average_valid_response_reward\n",
    "  size_summary['average strict format reward'] = average_strict_format_reward\n",
    "  size_summary['average soft format reward'] = average_soft_format_reward\n",
    "\n",
    "  # Append size summary to summary\n",
    "  summary[size] = size_summary\n",
    "\n",
    "# Append summary to results\n",
    "results['summary'] = summary\n",
    "\n",
    "# Save benchmark results\n",
    "with open(\"tsp_benchmark_results.json\", \"w\") as file:\n",
    "  json.dump(results, file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Final ENV)",
   "language": "python",
   "name": "final_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
